import streamlit as st
import random

# Definir el laberinto como una matriz con emojis
maze = [
    ['ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦'],
    ['ğŸŸ¦', 'ğŸš¶â€â™‚ï¸', ' ', ' ', 'ğŸŸ¦', ' ', ' ', ' ', 'ğŸŸ¦'],
    ['ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', ' ', 'ğŸŸ¦', ' ', 'ğŸŸ¦', ' ', 'ğŸŸ¦'],
    ['ğŸŸ¦', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'ğŸŸ¦'],
    ['ğŸŸ¦', ' ', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦'],
    ['ğŸŸ¦', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'ğŸŸ¦'],
    ['ğŸŸ¦', ' ', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', ' ', 'ğŸŸ¦', ' ', 'ğŸŸ¦'],
    ['ğŸŸ¦', ' ', ' ', ' ', ' ', ' ', ' ', 'ğŸ', 'ğŸŸ¦'],
    ['ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦', 'ğŸŸ¦']
]

# Definir las acciones posibles del agente como Ã­ndices (arriba, abajo, izquierda, derecha)
UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3
actions = [UP, DOWN, LEFT, RIGHT]

# Crear controles en el panel lateral
st.sidebar.title("ConfiguraciÃ³n de Q-Learning")
learning_rate = st.sidebar.slider("Tasa de Aprendizaje", 0.0, 1.0, 0.1, step=0.01)
discount_factor = st.sidebar.slider("Factor de Descuento", 0.0, 1.0, 0.9, step=0.01)
exploration_prob = st.sidebar.slider("Probabilidad de ExploraciÃ³n", 0.0, 1.0, 0.2, step=0.01)
num_episodes = st.sidebar.slider("NÃºmero de Episodios", 1, 5000, 1000)

# FunciÃ³n para verificar si una posiciÃ³n es vÃ¡lida en el laberinto
def is_valid(x, y):
    return 0 <= x < len(maze) and 0 <= y < len(maze[0]) and maze[x][y] != 'ğŸŸ¦'

# FunciÃ³n para realizar el aprendizaje Q-learning
def q_learning(maze, learning_rate, discount_factor, exploration_prob, num_episodes):
    q_table = {}
    for episode in range(num_episodes):
        x, y = 1, 1  # Iniciar en el punto de inicio (ğŸš¶â€â™‚ï¸)
        while maze[x][y] != 'ğŸ':
            if random.uniform(0, 1) < exploration_prob:
                action = random.choice(actions)
            else:
                action = max(actions, key=lambda a: q_table.get(((x, y), a), 0))

            dx, dy = 0, 0
            if action == UP:
                dx, dy = -1, 0
            elif action == DOWN:
                dx, dy = 1, 0
            elif action == LEFT:
                dx, dy = 0, -1
            elif action == RIGHT:
                dx, dy = 0, 1

            new_x, new_y = x + dx, y + dy

            if is_valid(new_x, new_y):
                next_max_q = max(q_table.get(((new_x, new_y), a), 0) for a in actions)
                reward = -1 if maze[new_x][new_y] != 'ğŸ' else 100  # Recompensa positiva al llegar a la meta
                q_table[((x, y), action)] = q_table.get(((x, y), action), 0) + \
                                             learning_rate * (reward + discount_factor * next_max_q - q_table.get(((x, y), action), 0))
                x, y = new_x, new_y

    return q_table

# FunciÃ³n para seguir la polÃ­tica Ã³ptima aprendida por el agente
def follow_policy(q_table, maze):
    x, y = 1, 1
    path = []
    while maze[x][y] != 'ğŸ':
        action = max(actions, key=lambda a: q_table.get(((x, y), a), 0))
        dx, dy = 0, 0
        if action == UP:
            dx, dy = -1, 0
        elif action == DOWN:
            dx, dy = 1, 0
        elif action == LEFT:
            dx, dy = 0, -1
        elif action == RIGHT:
            dx, dy = 0, 1
        new_x, new_y = x + dx, y + dy
        path.append((x, y))
        x, y = new_x, new_y
    path.append((x, y))
    return path

# Crear la aplicaciÃ³n Streamlit
st.title("SimulaciÃ³n de Q-Learning en un Laberinto")

# Entrenar al agente Q-learning con los valores seleccionados en el panel lateral
q_table = q_learning(maze, learning_rate, discount_factor, exploration_prob, num_episodes)

# Crear una tabla para mostrar el laberinto
maze_table = []
for row in maze:
    maze_table.append(['' if cell != 'ğŸŸ¦' else 'ğŸŸ¦' for cell in row])

# Marcar el camino Ã³ptimo en la tabla del laberinto
optimal_path = follow_policy(q_table, maze)
for x, y in optimal_path:
    maze_table[x][y] = 'ğŸš€'

# Mostrar el laberinto con el camino Ã³ptimo
st.markdown("## Laberinto con Camino Ã“ptimo")
st.table(maze_table)